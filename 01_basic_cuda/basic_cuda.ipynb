{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CUDA BASICS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add CUDA to path in Jupyter Notebook even though nvcc compiler detected in terminal, as it is not directly detected by ipykernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Sep_12_02:18:05_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.77\n",
      "Build cuda_12.6.r12.6/compiler.34841621_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "\n",
    "# Verify nvcc is now accessible\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **01 - CUDA Device Properties**\n",
    "\n",
    "Open the file [01_device_details.cu](./01_device_details.cu) to see the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o 01_device_details 01_device_details.cu\n",
      "./01_device_details\n",
      "\n",
      "Number of CUDA devices: 1\n",
      "Device #0: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "  Compute Capability: 8.6\n",
      "  Total Global Memory: 6.44193 GB\n",
      "  Shared Memory per Block: 48 KB\n",
      "  Registers per Block: 65536\n",
      "  Warp Size: 32\n",
      "  Max Threads per Block: 1024\n",
      "  Number of SMs: 30\n",
      "  Clock Rate: 1.425 GHz\n",
      "  Max Threads Dimension: [1024, 1024, 64]\n",
      "  Max Grid Size: [2147483647, 65535, 65535]\n",
      "  L2 Cache Size: 3072 KB\n",
      "  Memory Clock Rate: 7001 MHz\n",
      "  Memory Bus Width: 192 bits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make SRC=01_device_details.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Basic Properties\n",
    "- Device Name: Name of the GPU device.\n",
    "  - Example: `NVIDIA A6000`, `GeForce GTX 1080 Ti`.\n",
    "- Compute Capability: Indicates the architecture and feature set supported by the GPU.\n",
    "  - Format: `major.minor` (e.g., `7.5` for Turing, `8.0` for Ampere).\n",
    "  - Determines compatibility with CUDA features.\n",
    "\n",
    "### 2. Hardware Specifications\n",
    "- Number of Multiprocessors (SMs): Number of Streaming Multiprocessors.\n",
    "  - Higher SM count generally means higher parallelism.\n",
    "- Max Threads Per Block: Maximum number of threads allowed per block.\n",
    "  - Typical values: `1024`, `1536`.\n",
    "- Max Threads Per Multiprocessor: Maximum threads an SM can handle concurrently.\n",
    "  - Dependent on the architecture (e.g., `2048` for Volta, `1536` for Pascal).\n",
    "- Max Blocks Per SM: Maximum number of thread blocks an SM can run simultaneously.\n",
    "\n",
    "### 3. Memory Properties\n",
    "- Global Memory: Total memory available on the GPU device.\n",
    "  - Example: `48GB` for A6000, `8GB` for GTX 1080 Ti.\n",
    "  - Used for data transfer between host and device.\n",
    "- Shared Memory Per Block: Memory shared among threads in a block.\n",
    "  - Example: `48KB` or `100KB` (depending on architecture and configuration).\n",
    "- Total Shared Memory Per SM: Total shared memory available to an SM.\n",
    "- L1 Cache/Shared Memory Configurable: Ability to partition shared memory and L1 cache.\n",
    "  - Example: 16KB L1, 48KB shared or vice versa.\n",
    "- Registers Per Block: Maximum number of registers available per block.\n",
    "- Constant Memory: Read-only memory optimized for frequently used constants.\n",
    "  - Typically `64KB`.\n",
    "\n",
    "### 4. Execution Capabilities\n",
    "- Warp Size: Number of threads in a warp.\n",
    "  - Typically `32` for all NVIDIA GPUs.\n",
    "- Max Grid Dimensions: Maximum dimensions of a grid.\n",
    "  - Example: `(2^31 - 1, 65535, 65535)` in the X, Y, Z dimensions.\n",
    "- Max Block Dimensions: Maximum dimensions of a block.\n",
    "  - Example: `(1024, 1024, 64)` in X, Y, Z dimensions.\n",
    "\n",
    "### 5. Performance Metrics\n",
    "- Clock Rate: GPU core clock speed in kHz.\n",
    "  - Example: `1410 MHz`.\n",
    "  - Affects computation speed.\n",
    "- Memory Clock Rate: Speed of the GPU memory in kHz.\n",
    "  - Example: `6 GHz` for GDDR6.\n",
    "- Memory Bus Width: Width of the memory bus in bits.\n",
    "  - Example: `384-bit`.\n",
    "- Peak Memory Bandwidth: Maximum memory transfer rate.\n",
    "  - Example: `936 GB/s`.\n",
    "\n",
    "### 6. Concurrency Features\n",
    "- Concurrent Kernels: Indicates if multiple kernels can execute simultaneously.\n",
    "- Async Engine Count: Number of asynchronous engines for concurrent copy and execution.\n",
    "- Overlap: Ability to overlap data transfer and kernel execution.\n",
    "\n",
    "### 7. Unified Addressing\n",
    "- Unified Memory: Indicates support for unified memory, allowing shared memory between host and device.\n",
    "- Managed Memory: Support for memory managed automatically by CUDA.\n",
    "\n",
    "### 8. Special Capabilities\n",
    "- Tensor Cores: Present in GPUs with compute capability `7.0` and above (e.g., Turing, Ampere).\n",
    "  - Accelerates deep learning matrix operations.\n",
    "- Ray Tracing Cores: Present in RTX GPUs for real-time ray tracing applications.\n",
    "- FP16 and FP64 Performance: Indicates support for 16-bit and 64-bit floating-point operations.\n",
    "  - Double precision (`FP64`) is slower on consumer GPUs compared to professional GPUs (e.g., A6000).\n",
    "\n",
    "### 9. Others\n",
    "- ECC Support: Indicates whether Error Correcting Code (ECC) memory is available.\n",
    "  - Critical for scientific and financial computations.\n",
    "- Device Overlap: If device can overlap computation and data transfer.\n",
    "- CUDA Version: Supported CUDA runtime version.\n",
    "\n",
    "### How to Use This Information\n",
    "- Optimize Kernel Performance:\n",
    "  - Design kernels to utilize shared memory efficiently.\n",
    "  - Use appropriate thread/block configurations within the device limits.\n",
    "- Memory Bandwidth:\n",
    "  - Use coalesced memory access patterns to improve bandwidth utilization.\n",
    "- Concurrency:\n",
    "  - Use streams for overlapping data transfer and computation.\n",
    "- Deep Learning:\n",
    "  - Leverage Tensor Cores for matrix multiplication if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f 01_device_details\n"
     ]
    }
   ],
   "source": [
    "!make SRC=01_device_details.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **02 - Generating threads and blocks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **03 - Data movement**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
