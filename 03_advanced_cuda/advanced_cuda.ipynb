{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced CUDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add CUDA to path in Jupyter Notebook even though nvcc compiler is detected in terminal, as it is not directly detected by ipykernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "\n",
    "# Verify nvcc is now accessible\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **01 - Atomic Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An atomic operation in CUDA is a type of operation that is performed in a way that ensures it is indivisible—that is, it cannot be interrupted or affected by other threads. When multiple threads attempt to modify a shared memory location, atomic operations ensure that these modifications are executed one at a time, avoiding race conditions.\n",
    "\n",
    "For example, when multiple threads try to increment a shared counter, an atomic operation ensures that each increment happens sequentially, even if threads are running concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Are Atomic Operations Necessary?**\n",
    "\n",
    "In parallel programming, multiple threads often need to access or update shared data. Without synchronization mechanisms like atomic operations, the following issues can arise:\n",
    "- Race Conditions: Multiple threads attempt to update the same variable simultaneously, leading to inconsistent results.\n",
    "- Data Corruption: Intermediate results of one thread's operation can be overwritten by another thread.\n",
    "- Incorrect Computation: Operations that depend on shared data (e.g., summation, counting) may produce wrong results due to simultaneous accesses.\n",
    "\n",
    "Atomic operations prevent these problems by serializing access to the shared resource, ensuring that only one thread modifies the variable at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Atomic Operations in CUDA**\n",
    "\n",
    "CUDA provides several atomic functions that operate on different data types and perform common operations:\n",
    "\n",
    "- Arithmetic Operations:\n",
    "    - atomicAdd: Adds a value to a shared variable.\n",
    "    - atomicSub: Subtracts a value from a shared variable.\n",
    "    - atomicExch: Replaces a value with a new one.\n",
    "\n",
    "- Comparison and Logical Operations:\n",
    "    - atomicMin: Updates the variable with the minimum of the current and provided value.\n",
    "    - atomicMax: Updates the variable with the maximum of the current and provided value.\n",
    "    - atomicCAS (Compare and Swap): Updates a variable only if it equals a specified value.\n",
    "\n",
    "- Bitwise Operations:\n",
    "    - atomicAnd: Performs a bitwise AND.\n",
    "    - atomicOr: Performs a bitwise OR.\n",
    "    - atomicXor: Performs a bitwise XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding array of 1024 1s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/01_atomic_operations ./src/01_atomic_operations.cu\n",
      "././src/01_atomic_operations\n",
      "Sum of array elements (normalSumKernel): 1\n",
      "Sum of array elements (atomicSumKernel): 1024\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/01_atomic_operations.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "- Normal Sum (`normalSumKernel`):\n",
    "Each thread reads from the input array and adds its value to the shared variable result.\n",
    "Issue: Without atomic operations, multiple threads may update result simultaneously, leading to race conditions and an incorrect sum.\n",
    "\n",
    "- Atomic Sum (`atomicSumKernel`):\n",
    "Uses `atomicAdd` to safely add each thread’s contribution to result.\n",
    "Solution: Ensures that only one thread updates result at a time, preventing race conditions and producing the correct sum.\n",
    "\n",
    "- Result Comparison:\n",
    "`normalSumKernel`: Results are incorrect because threads overwrite each other's updates.\n",
    "`atomicSumKernel`: Produces the correct sum by using atomic operations to serialize updates.\n",
    "\n",
    "**Why Atomic Operations Are Crucial in This Code?**\n",
    "\n",
    "- The shared variable result is updated concurrently by multiple threads.\n",
    "- Without atomicAdd, updates are not safe in parallel, leading to data corruption.\n",
    "- `atomicAdd` ensures correctness but can slow performance due to thread serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counter using Normal Addition and Atomic Addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/01_atomic_operations\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/01_atomic_operations.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **02 - Events**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA events are a mechanism in the CUDA API used to measure the time taken by operations on the GPU or to synchronize operations between different streams. CUDA events are lightweight and designed specifically for timing and synchronization tasks in GPU programming.\n",
    "Necessity of CUDA Events\n",
    "\n",
    "- Performance Measurement: CUDA events allow you to measure the execution time of GPU operations accurately.\n",
    "- Synchronization: Events can synchronize streams or host-device operations without blocking the entire application.\n",
    "- Granular Timing: They provide more precise control and insight compared to cudaDeviceSynchronize or host-based timers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/02_events ./src/02_events.cu\n",
      "././src/02_events\n",
      "Kernel execution time: 0.902464 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/02_events.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Events:**\n",
    "\n",
    "`cudaEvent_t start, stop;`\n",
    "\n",
    "`cudaEventCreate(&start);`\n",
    "\n",
    "`cudaEventCreate(&stop);`\n",
    "\n",
    "- `start` and `stop` are event handles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Events:**\n",
    "\n",
    "`cudaEventRecord(start);`\n",
    "\n",
    "- Start recording before the kernel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synchronize Events:**\n",
    "\n",
    "`cudaEventSynchronize(stop);`\n",
    "\n",
    "- Ensures all operations before stop are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Elapsed Time:**\n",
    "\n",
    "`cudaEventElapsedTime(&milliseconds, start, stop);`\n",
    "\n",
    "- Computes time in milliseconds between the start and stop events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Destroy Events:**\n",
    "\n",
    "`cudaEventDestroy(start);`\n",
    "\n",
    "`cudaEventDestroy(stop);`\n",
    "\n",
    "- Cleans up event resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/02_events\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/02_events.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **03 - Streams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Streams: Enhancing GPU Parallelism**\n",
    "\n",
    "CUDA streams are a powerful feature in NVIDIA's CUDA programming model that allow for concurrent execution of operations on the GPU. They provide an additional layer of parallelism beyond the traditional thread and block model, enabling more efficient utilization of GPU resources.\n",
    "- Key Concepts\n",
    "\n",
    "    - Definition: A CUDA stream is a sequence of operations that execute on the GPU in a specific order.\n",
    "    - Purpose: Streams enable concurrent execution of kernels and memory transfers, improving overall performance2.\n",
    "    - Default Stream: All CUDA operations occur in the default stream if not specified otherwise1.\n",
    "\n",
    "- Stream Behavior\n",
    "\n",
    "    - Ordering: Operations within a single stream are executed sequentially.\n",
    "    - Concurrency: Different non-default streams can execute operations concurrently.\n",
    "    - Default Stream Behavior: The default stream is blocking and synchronizes with all other streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/cuda_streams.bmp\" alt=\"CUDA Streams\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the performance difference between serial and concurrent CUDA stream execution.\n",
    "\n",
    "- The top portion shows a Serial execution where operations happen sequentially:\n",
    "\n",
    "    1. Memory copy from Host to Device (H2D)\n",
    "    2. Kernel execution\n",
    "    3. Memory copy from Device to Host (D2H)\n",
    "\n",
    "- The bottom portion shows Concurrent execution using three streams:\n",
    "\n",
    "    - Stream 1, 2, and 3 execute their operations (H2D, Kernel, D2H) in parallel\n",
    "    - Operations within each stream remain sequential\n",
    "    - Streams are staggered in time, allowing overlap of different operations\n",
    "\n",
    "- The red dotted lines highlight the Performance improvement achieved through concurrent execution, showing how parallel streams complete the same workload in less time compared to serial execution. The green boxes represent memory transfers (H2D and D2H), while the blue boxes represent kernel executions. \n",
    "\n",
    "This visualization effectively demonstrates how CUDA streams can improve GPU utilization by overlapping computation and data transfer operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of Using Streams**\n",
    "\n",
    "- Improved GPU Utilization: Overlapping kernel execution with data transfers.\n",
    "- Reduced Idle Time: Keeping the GPU busy with multiple concurrent operations.\n",
    "- Enhanced Performance: Achieving higher throughput for certain workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stream Create, Synchronize, and Destroy Syntax**\n",
    "\n",
    "You can declare a stream with `cudaStream_t`, create it using `cudaStreamCreate(&stream)`. To synchronize a stream, use `cudaStreamSynchronize(stream)`, ensuring that all tasks in the stream finish before proceeding. Finally, free resources with `cudaStreamDestroy(stream)`.\n",
    "\n",
    "The `cudaStreamSynchronize` function is a crucial synchronization tool that blocks the host thread until all previously queued operations in the specified stream complete their execution.\n",
    "\n",
    "Usage Scenarios:\n",
    "- Ensuring data consistency before host access.\n",
    "- Coordinating multiple stream operations.\n",
    "- Managing dependencies between CPU and GPU tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax to launch kernel in a Stream**\n",
    "\n",
    "`myKernel<<<gridSize, blockSize, sharedMem, stream>>>(parameters);`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `gridSize`: Specifies the number of thread blocks in the grid.\n",
    "- `blockSize`: Defines the number of threads in each block.\n",
    "- `sharedMem`: Amount of shared memory to allocate per block (in bytes).\n",
    "- `stream`: Specifies which CUDA stream will execute this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - Without CUDA Stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03a_no_streams ./src/03a_no_streams.cu\n",
      "././src/03a_no_streams\n",
      "Execution time without streams: 1.1448 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03a_no_streams.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - With CUDA Stream (Individually created streams)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03b_with_streams ./src/03b_with_streams.cu\n",
      "././src/03b_with_streams\n",
      "Execution time with streams: 2.86925 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03b_with_streams.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - With CUDA Stream (Streams created in `for` loop)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03c_with_streams_for ./src/03c_with_streams_for.cu\n",
      "././src/03c_with_streams_for\n",
      "Execution time with streams: 2.4617 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03c_with_streams_for.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speedup occurs because:\n",
    "\n",
    "- The workload is divided into 4 independent streams\n",
    "- Memory transfers and kernel executions overlap across streams\n",
    "- While one stream is executing its kernel, another stream can be performing memory transfers\n",
    "- The GPU's hardware resources are utilized more efficiently through concurrent execution\n",
    "\n",
    "This example demonstrates how CUDA streams can significantly improve performance by enabling parallel execution of operations that would otherwise need to wait for previous operations to complete in a serial implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/03a_no_streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/03b_with_streams\n",
      "rm -f ./src/03c_with_streams_for\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03a_no_streams.cu clean\n",
    "!make SRC=./src/03b_with_streams.cu clean\n",
    "!make SRC=./src/03c_with_streams_for.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **04 - Memory Coalescing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images/uncoalesced_memory.png\" alt=\"Uncoalesced Memory Access\" width=\"30%\">\n",
    "    <img src=\"./images/coalesced_memory.png\" alt=\"Coalesced Memory Access\" width=\"30%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory coalescing is a crucial optimization technique in parallel computing, particularly for GPU architectures like CUDA. It refers to the process of merging multiple memory accesses into a single, larger access to improve data transfer efficiency. This technique is especially important for optimizing performance in systems with hierarchical memory models, where non-coalesced memory access can lead to significant performance penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Memory Coalescing Matters**\n",
    "The importance of memory coalescing stems from the nature of modern DRAM (Dynamic Random-Access Memory) used in GPU global memory:\n",
    "- DRAM Characteristics: DRAM is relatively slow compared to processor speeds. Reading data from DRAM cells involves a time-consuming process of detecting tiny electrical charges.\n",
    "- Parallelism in Memory Access: To compensate for this slowness, modern DRAMs use parallelism to increase memory access throughput.\n",
    "- DRAM Bursts: When a DRAM location is accessed, a range of consecutive locations is actually read. This is known as a DRAM burst, which allows for high-speed data transfer once the initial access is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How Memory Coalescing Works**\n",
    "Memory coalescing takes advantage of the parallel nature of GPU processing:\n",
    "- Warp Execution: In CUDA architectures, threads within a warp execute the same instruction at any given time.\n",
    "- Consecutive Access Pattern: The most favorable access pattern occurs when all threads in a warp access consecutive global memory locations.\n",
    "- Hardware Detection: When threads in a warp execute a load instruction, the hardware detects if they are accessing consecutive memory locations.\n",
    "- Consolidated Access: If consecutive access is detected, the hardware coalesces these accesses into a single consolidated request for consecutive DRAM locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of Memory Coalescing**\n",
    "\n",
    "Memory coalescing offers several advantages:\n",
    "- Improved Performance: It allows the system to handle multiple memory requests in fewer cycles, significantly enhancing overall performance.\n",
    "- Efficient Bandwidth Utilization: By consolidating memory accesses, coalescing makes more efficient use of the available memory bandwidth.\n",
    "- Reduced Latency: Coalesced access patterns can help reduce memory access latency, as fewer separate memory transactions are required.\n",
    "\n",
    "By understanding and implementing memory coalescing techniques, developers can significantly optimize the performance of parallel computing applications, especially those running on GPU architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row-Major vs. Column-Major Order**\n",
    "\n",
    "For row-major matrices, row-wise access typically yields better performance due to contiguous memory reads, resulting in coalesced memory transactions. Conversely, column-wise access can lead to strided memory patterns, potentially causing uncoalesced memory operations and reduced efficiency. For column-major matrices, the opposite is true: column-wise access provides better coalescing. The key is aligning the access pattern with the storage order to maximize memory bandwidth utilization and minimize cache misses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row Major (Column-Wise) Matrix Addition - Non Coalesced Memory Access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/04a_column_wise_add ./src/04a_column_wise_add.cu\n",
      "././src/04a_column_wise_add\n",
      "Column-wise matrix addition completed in 8.12403 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/04a_column_wise_add.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Row-Major (Row-wise) Matrix Addition - Coalesced Memory Access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/04b_row_wise_add ./src/04b_row_wise_add.cu\n",
      "././src/04b_row_wise_add\n",
      "Row-major matrix addition completed in 7.05098 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/04b_row_wise_add.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/04a_column_wise_add\n",
      "rm -f ./src/04b_row_wise_add\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/04a_column_wise_add.cu clean\n",
    "!make SRC=./src/04b_row_wise_add.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **05 - Shared Memory Bank Conflict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared memory in NVIDIA GPUs is divided into 32 banks, each 4 bytes wide, designed for parallel access by threads in a warp. Access is efficient if threads target different banks, but bank conflicts occur when multiple threads access the same bank, causing serialized transactions and reduced performance. Avoid conflicts using padding or data alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared Memory Bank Access Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"./images/no_conflict1.png\" alt=\"Uncoalesced Memory Access\" width=\"30%\">\n",
    "    <img src=\"./images/no_conflict2.png\" alt=\"Coalesced Memory Access\" width=\"30%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From left to right:\n",
    "\n",
    "1. Linear addressing with a stride of one 32-bit word (no bank conflict).\n",
    "2. Linear addressing with a stride of two 32-bit words (two-way bank conflict).\n",
    "3. Linear addressing with a stride of three 32-bit words (no bank conflict).\n",
    "4. Conflict-free access via random permutation.\n",
    "5. Conflict-free access since threads 3, 4, 6, 7, and 9 access the same word within bank 5.\n",
    "6. Conflict-free broadcast access (threads access the same word within a bank).\n",
    "\n",
    "Bank conflicts occur in shared memory when multiple threads in a warp access different addresses within the same memory bank, except when accessing the same address (broadcast). Linear addressing with a stride of one or three 32-bit words and conflict-free permutations avoid conflicts, while a stride of two words creates two-way conflicts, and broadcasts inherently remain conflict-free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avoiding Bank Conflicts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/bank_conflict_padding.png\" alt=\"Avoiding Bank Conflict with Padding\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Padding: Adding extra memory to shift indexing and avoid conflicts.\n",
    "- Alignment: Structuring data to align with bank boundaries.\n",
    "- Strided Access Management: Ensuring that threads do not access addresses with strides that cause conflicts.\n",
    "- Conflict-Free Permutations: Reorganizing memory indices for conflict-free access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Transpose with Bank Conflicts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line `d_out[y * rows + x] = tile[tid_x][tid_y];` causes a bank conflict when multiple threads in a warp access the same column (`tid_y)` of the shared memory tile (`tile[tid_x][tid_y]`). This happens because shared memory is divided into banks, and simultaneous accesses to the same bank by different threads cause delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/05a_transpose_conflict ./src/05a_transpose_conflict.cu\n",
      "././src/05a_transpose_conflict\n",
      "Matrix transposition verified successfully!\n",
      "Execution time: 0.763904 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/05a_transpose_conflict.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix Transpose with Bank Padding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__shared__ float tile[32][32 + 1];`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The +1 offsets each row by one element, ensuring that accesses are distributed across different banks, avoiding conflicts, and enabling efficient parallel access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/05b_transpose_padding ./src/05b_transpose_padding.cu\n",
      "././src/05b_transpose_padding\n",
      "Matrix transposition verified successfully!\n",
      "Execution time: 0.208896 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/05b_transpose_padding.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/05a_transpose_conflict\n",
      "rm -f ./src/05b_transpose_padding\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/05a_transpose_conflict.cu clean\n",
    "!make SRC=./src/05b_transpose_padding.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **06 - Warp Divergence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/warp_divergence.bmp\" alt=\"Warp Divergence\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warp divergence occurs when threads in a warp (group of 32 threads in CUDA) follow different execution paths due to conditional branches. For example, if some threads in a warp take one branch of an if statement while others take a different branch, the warp must execute both paths sequentially rather than in parallel, causing a performance penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is it a Problem?**\n",
    "\n",
    "- Reduced Parallelism: Ideally, all threads in a warp should execute the same instruction at the same time. With divergence, different threads execute different instructions, which reduces the level of parallelism.\n",
    "- Performance Penalty: The GPU must serialize execution of divergent branches, executing one path at a time for all threads in the warp, leading to inefficient use of resources and slower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions to Warp Divergence:**\n",
    "\n",
    "- Minimize Conditional Branching: Try to avoid if statements that cause divergent paths, especially within the same warp.\n",
    "- Use Predication: Convert conditional branches into predicated operations where all threads execute the same instructions but only the relevant threads do useful work based on the condition.\n",
    "- Reorganize Code: Where possible, refactor code so that threads within the same warp are less likely to diverge. For example, you could rearrange data or loop structures to ensure uniform behavior across threads.\n",
    "- Use Warp-Synchronous Programming: If divergence is unavoidable, try to ensure that divergent branches are as small as possible and isolate them from the rest of the warp execution.\n",
    "\n",
    "By reducing warp divergence, you improve the parallel efficiency and overall performance of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code with Warp Divergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/06a_warp_divergence ./src/06a_warp_divergence.cu\n",
      "././src/06a_warp_divergence\n",
      "Time taken with warp divergence: 4.97 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/06a_warp_divergence.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warp divergence occurs because threads within a warp evaluate the condition `if (d_in[idx] % 2 == 0)` differently, causing them to take different execution paths (if or else). CUDA executes these branches sequentially, disabling threads not active in the current branch, which reduces parallel efficiency. This happens because warps execute instructions in lockstep, and divergence forces them to serialize execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solving Warp Divergence by Predicating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/06b_warp_predicate ./src/06b_warp_predicate.cu\n",
      "././src/06b_warp_predicate\n",
      "Time taken without warp divergence: 4.94 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/06b_warp_predicate.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach eliminates warp divergence by ensuring all threads in a warp execute the same instruction. Instead of branching with if-else, the ternary operator computes both outcomes and uses the mask to select the correct result for each thread. Since all threads follow the same execution path regardless of the condition, the GPU maintains full warp efficiency without serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/06a_warp_divergence\n",
      "rm -f ./src/06b_warp_predicate\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/06a_warp_divergence.cu clean\n",
    "!make SRC=./src/06b_warp_predicate.cu clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
