{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced CUDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add CUDA to path in Jupyter Notebook even though nvcc compiler is detected in terminal, as it is not directly detected by ipykernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Oct_29_23:50:19_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "\n",
    "# Verify nvcc is now accessible\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **01 - Atomic Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An atomic operation in CUDA is a type of operation that is performed in a way that ensures it is indivisible—that is, it cannot be interrupted or affected by other threads. When multiple threads attempt to modify a shared memory location, atomic operations ensure that these modifications are executed one at a time, avoiding race conditions.\n",
    "\n",
    "For example, when multiple threads try to increment a shared counter, an atomic operation ensures that each increment happens sequentially, even if threads are running concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Are Atomic Operations Necessary?**\n",
    "\n",
    "In parallel programming, multiple threads often need to access or update shared data. Without synchronization mechanisms like atomic operations, the following issues can arise:\n",
    "- Race Conditions: Multiple threads attempt to update the same variable simultaneously, leading to inconsistent results.\n",
    "- Data Corruption: Intermediate results of one thread's operation can be overwritten by another thread.\n",
    "- Incorrect Computation: Operations that depend on shared data (e.g., summation, counting) may produce wrong results due to simultaneous accesses.\n",
    "\n",
    "Atomic operations prevent these problems by serializing access to the shared resource, ensuring that only one thread modifies the variable at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Atomic Operations in CUDA**\n",
    "\n",
    "CUDA provides several atomic functions that operate on different data types and perform common operations:\n",
    "\n",
    "- Arithmetic Operations:\n",
    "    - atomicAdd: Adds a value to a shared variable.\n",
    "    - atomicSub: Subtracts a value from a shared variable.\n",
    "    - atomicExch: Replaces a value with a new one.\n",
    "\n",
    "- Comparison and Logical Operations:\n",
    "    - atomicMin: Updates the variable with the minimum of the current and provided value.\n",
    "    - atomicMax: Updates the variable with the maximum of the current and provided value.\n",
    "    - atomicCAS (Compare and Swap): Updates a variable only if it equals a specified value.\n",
    "\n",
    "- Bitwise Operations:\n",
    "    - atomicAnd: Performs a bitwise AND.\n",
    "    - atomicOr: Performs a bitwise OR.\n",
    "    - atomicXor: Performs a bitwise XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/01_atomic_operations ./src/01_atomic_operations.cu\n",
      "././src/01_atomic_operations\n",
      "Sum of array elements (normalSumKernel): 1\n",
      "Sum of array elements (atomicSumKernel): 1024\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/01_atomic_operations.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "- Normal Sum (`normalSumKernel`):\n",
    "Each thread reads from the input array and adds its value to the shared variable result.\n",
    "Issue: Without atomic operations, multiple threads may update result simultaneously, leading to race conditions and an incorrect sum.\n",
    "\n",
    "- Atomic Sum (`atomicSumKernel`):\n",
    "Uses `atomicAdd` to safely add each thread’s contribution to result.\n",
    "Solution: Ensures that only one thread updates result at a time, preventing race conditions and producing the correct sum.\n",
    "\n",
    "- Result Comparison:\n",
    "`normalSumKernel`: Results are incorrect because threads overwrite each other's updates.\n",
    "`atomicSumKernel`: Produces the correct sum by using atomic operations to serialize updates.\n",
    "\n",
    "**Why Atomic Operations Are Crucial in This Code?**\n",
    "\n",
    "- The shared variable result is updated concurrently by multiple threads.\n",
    "- Without atomicAdd, updates are not safe in parallel, leading to data corruption.\n",
    "- `atomicAdd` ensures correctness but can slow performance due to thread serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counter using Normal Addition and Atomic Addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/01_atomic_operations\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/01_atomic_operations.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **02 - Events**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA events are a mechanism in the CUDA API used to measure the time taken by operations on the GPU or to synchronize operations between different streams. CUDA events are lightweight and designed specifically for timing and synchronization tasks in GPU programming.\n",
    "Necessity of CUDA Events\n",
    "\n",
    "- Performance Measurement: CUDA events allow you to measure the execution time of GPU operations accurately.\n",
    "- Synchronization: Events can synchronize streams or host-device operations without blocking the entire application.\n",
    "- Granular Timing: They provide more precise control and insight compared to cudaDeviceSynchronize or host-based timers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/02_events ./src/02_events.cu\n",
      "././src/02_events\n",
      "Kernel execution time: 474.403 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/02_events.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Events:**\n",
    "\n",
    "`cudaEvent_t start, stop;`\n",
    "\n",
    "`cudaEventCreate(&start);`\n",
    "\n",
    "`cudaEventCreate(&stop);`\n",
    "\n",
    "- `start` and `stop` are event handles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Record Events:**\n",
    "\n",
    "`cudaEventRecord(start);`\n",
    "\n",
    "- Start recording before the kernel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synchronize Events:**\n",
    "\n",
    "`cudaEventSynchronize(stop);`\n",
    "\n",
    "- Ensures all operations before stop are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Elapsed Time:**\n",
    "\n",
    "`cudaEventElapsedTime(&milliseconds, start, stop);`\n",
    "\n",
    "- Computes time in milliseconds between the start and stop events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Destroy Events:**\n",
    "\n",
    "`cudaEventDestroy(start);`\n",
    "\n",
    "`cudaEventDestroy(stop);`\n",
    "\n",
    "- Cleans up event resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/02_events\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/02_events.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **03 - Streams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Streams: Enhancing GPU Parallelism**\n",
    "\n",
    "CUDA streams are a powerful feature in NVIDIA's CUDA programming model that allow for concurrent execution of operations on the GPU. They provide an additional layer of parallelism beyond the traditional thread and block model, enabling more efficient utilization of GPU resources.\n",
    "- Key Concepts\n",
    "\n",
    "    - Definition: A CUDA stream is a sequence of operations that execute on the GPU in a specific order.\n",
    "    - Purpose: Streams enable concurrent execution of kernels and memory transfers, improving overall performance2.\n",
    "    - Default Stream: All CUDA operations occur in the default stream if not specified otherwise1.\n",
    "\n",
    "- Stream Behavior\n",
    "\n",
    "    - Ordering: Operations within a single stream are executed sequentially.\n",
    "    - Concurrency: Different non-default streams can execute operations concurrently.\n",
    "    - Default Stream Behavior: The default stream is blocking and synchronizes with all other streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/cuda_streams.bmp\" alt=\"CUDA Streams\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image illustrates the performance difference between serial and concurrent CUDA stream execution.\n",
    "\n",
    "- The top portion shows a Serial execution where operations happen sequentially:\n",
    "\n",
    "    1. Memory copy from Host to Device (H2D)\n",
    "    2. Kernel execution\n",
    "    3. Memory copy from Device to Host (D2H)\n",
    "\n",
    "- The bottom portion shows Concurrent execution using three streams:\n",
    "\n",
    "    - Stream 1, 2, and 3 execute their operations (H2D, Kernel, D2H) in parallel\n",
    "    - Operations within each stream remain sequential\n",
    "    - Streams are staggered in time, allowing overlap of different operations\n",
    "\n",
    "- The red dotted lines highlight the Performance improvement achieved through concurrent execution, showing how parallel streams complete the same workload in less time compared to serial execution. The green boxes represent memory transfers (H2D and D2H), while the blue boxes represent kernel executions. \n",
    "\n",
    "This visualization effectively demonstrates how CUDA streams can improve GPU utilization by overlapping computation and data transfer operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of Using Streams**\n",
    "\n",
    "- Improved GPU Utilization: Overlapping kernel execution with data transfers.\n",
    "- Reduced Idle Time: Keeping the GPU busy with multiple concurrent operations.\n",
    "- Enhanced Performance: Achieving higher throughput for certain workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Syntax**\n",
    "\n",
    "`myKernel<<<gridSize, blockSize, sharedMem, stream>>>(parameters);`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `gridSize`: Specifies the number of thread blocks in the grid.\n",
    "- `blockSize`: Defines the number of threads in each block.\n",
    "- `sharedMem`: Amount of shared memory to allocate per block (in bytes).\n",
    "- `stream`: Specifies which CUDA stream will execute this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cuda Stream Synchronization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cudaStreamSynchronize(cudaStream_t stream);` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cudaStreamSynchronize function is a crucial synchronization tool that blocks the host thread until all previously queued operations in the specified stream complete their execution.\n",
    "\n",
    "Usage Scenarios:\n",
    "- Ensuring data consistency before host access.\n",
    "- Coordinating multiple stream operations.\n",
    "- Managing dependencies between CPU and GPU tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - Without CUDA Stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03a_no_streams ./src/03a_no_streams.cu\n",
      "././src/03a_no_streams\n",
      "Execution time without streams: 12.28 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03a_no_streams.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - With CUDA Stream (Individually created streams)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03b_with_streams ./src/03b_with_streams.cu\n",
      "././src/03b_with_streams\n",
      "Execution time with streams: 2.63987 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03b_with_streams.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squaring number - With CUDA Stream (Streams created in `for` loop)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/03c_with_streams_for ./src/03c_with_streams_for.cu\n",
      "././src/03c_with_streams_for\n",
      "Execution time with streams: 3.52768 ms\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03c_with_streams_for.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dramatic speedup from 12ms to ~2-4ms (~3x improvement) occurs because:\n",
    "\n",
    "- The workload is divided into 4 independent streams\n",
    "- Memory transfers and kernel executions overlap across streams\n",
    "- While one stream is executing its kernel, another stream can be performing memory transfers\n",
    "- The GPU's hardware resources are utilized more efficiently through concurrent execution\n",
    "\n",
    "This example demonstrates how CUDA streams can significantly improve performance by enabling parallel execution of operations that would otherwise need to wait for previous operations to complete in a serial implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/03a_no_streams\n",
      "rm -f ./src/03b_with_streams\n",
      "rm -f ./src/03c_with_streams_for\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/03a_no_streams.cu clean\n",
    "!make SRC=./src/03b_with_streams.cu clean\n",
    "!make SRC=./src/03c_with_streams_for.cu clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **04 - Memory Coalescing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **05 - Shared Memory Bank Conflict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **06 - Warp Divergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
