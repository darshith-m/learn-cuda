{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CUDA BASICS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add CUDA to path in Jupyter Notebook even though nvcc compiler is detected in terminal, as it is not directly detected by ipykernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += \":/usr/local/cuda/bin\"\n",
    "\n",
    "# Verify nvcc is now accessible\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **01 - CUDA Device Properties**\n",
    "\n",
    "Open the file [01_device_details.cu](./01_device_details.cu) to see the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/01_device_details.cu run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Basic Properties\n",
    "- Device Name: Name of the GPU device.\n",
    "  - Example: `NVIDIA A6000`, `GeForce GTX 1080 Ti`.\n",
    "- Compute Capability: Indicates the architecture and feature set supported by the GPU.\n",
    "  - Format: `major.minor` (e.g., `7.5` for Turing, `8.0` for Ampere).\n",
    "  - Determines compatibility with CUDA features.\n",
    "\n",
    "### 2. Hardware Specifications\n",
    "- Number of Multiprocessors (SMs): Number of Streaming Multiprocessors.\n",
    "  - Higher SM count generally means higher parallelism.\n",
    "- Max Threads Per Block: Maximum number of threads allowed per block.\n",
    "  - Typical values: `1024`, `1536`.\n",
    "- Max Threads Per Multiprocessor: Maximum threads an SM can handle concurrently.\n",
    "  - Dependent on the architecture (e.g., `2048` for Volta, `1536` for Pascal).\n",
    "- Max Blocks Per SM: Maximum number of thread blocks an SM can run simultaneously.\n",
    "\n",
    "### 3. Memory Properties\n",
    "- Global Memory: Total memory available on the GPU device.\n",
    "  - Example: `48GB` for A6000, `8GB` for GTX 1080 Ti.\n",
    "  - Used for data transfer between host and device.\n",
    "- Shared Memory Per Block: Memory shared among threads in a block.\n",
    "  - Example: `48KB` or `100KB` (depending on architecture and configuration).\n",
    "- Total Shared Memory Per SM: Total shared memory available to an SM.\n",
    "- L1 Cache/Shared Memory Configurable: Ability to partition shared memory and L1 cache.\n",
    "  - Example: 16KB L1, 48KB shared or vice versa.\n",
    "- Registers Per Block: Maximum number of registers available per block.\n",
    "- Constant Memory: Read-only memory optimized for frequently used constants.\n",
    "  - Typically `64KB`.\n",
    "\n",
    "### 4. Execution Capabilities\n",
    "- Warp Size: Number of threads in a warp.\n",
    "  - Typically `32` for all NVIDIA GPUs.\n",
    "- Max Grid Dimensions: Maximum dimensions of a grid.\n",
    "  - Example: `(2^31 - 1, 65535, 65535)` in the X, Y, Z dimensions.\n",
    "- Max Block Dimensions: Maximum dimensions of a block.\n",
    "  - Example: `(1024, 1024, 64)` in X, Y, Z dimensions.\n",
    "\n",
    "### 5. Performance Metrics\n",
    "- Clock Rate: GPU core clock speed in kHz.\n",
    "  - Example: `1410 MHz`.\n",
    "  - Affects computation speed.\n",
    "- Memory Clock Rate: Speed of the GPU memory in kHz.\n",
    "  - Example: `6 GHz` for GDDR6.\n",
    "- Memory Bus Width: Width of the memory bus in bits.\n",
    "  - Example: `384-bit`.\n",
    "- Peak Memory Bandwidth: Maximum memory transfer rate.\n",
    "  - Example: `936 GB/s`.\n",
    "\n",
    "### 6. Concurrency Features\n",
    "- Concurrent Kernels: Indicates if multiple kernels can execute simultaneously.\n",
    "- Async Engine Count: Number of asynchronous engines for concurrent copy and execution.\n",
    "- Overlap: Ability to overlap data transfer and kernel execution.\n",
    "\n",
    "### 7. Unified Addressing\n",
    "- Unified Memory: Indicates support for unified memory, allowing shared memory between host and device.\n",
    "- Managed Memory: Support for memory managed automatically by CUDA.\n",
    "\n",
    "### 8. Special Capabilities\n",
    "- Tensor Cores: Present in GPUs with compute capability `7.0` and above (e.g., Turing, Ampere).\n",
    "  - Accelerates deep learning matrix operations.\n",
    "- Ray Tracing Cores: Present in RTX GPUs for real-time ray tracing applications.\n",
    "- FP16 and FP64 Performance: Indicates support for 16-bit and 64-bit floating-point operations.\n",
    "  - Double precision (`FP64`) is slower on consumer GPUs compared to professional GPUs (e.g., A6000).\n",
    "\n",
    "### 9. Others\n",
    "- ECC Support: Indicates whether Error Correcting Code (ECC) memory is available.\n",
    "  - Critical for scientific and financial computations.\n",
    "- Device Overlap: If device can overlap computation and data transfer.\n",
    "- CUDA Version: Supported CUDA runtime version.\n",
    "\n",
    "### How to Use This Information\n",
    "- Optimize Kernel Performance:\n",
    "  - Design kernels to utilize shared memory efficiently.\n",
    "  - Use appropriate thread/block configurations within the device limits.\n",
    "- Memory Bandwidth:\n",
    "  - Use coalesced memory access patterns to improve bandwidth utilization.\n",
    "- Concurrency:\n",
    "  - Use streams for overlapping data transfer and computation.\n",
    "- Deep Learning:\n",
    "  - Leverage Tensor Cores for matrix multiplication if available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/01_device_details.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **02 - Kernels, Thread and Block Configuration** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./images/CUDA-GridBlockThread-Structure.png\" alt=\"Threads and blocks configuration\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining grid and block structure:\n",
    "    - dim3 is a built-in CUDA type that represents 3D vectors.\n",
    "    - threadsPerBlock specifies the dimensions of each block in terms of threads.\n",
    "    -  ```dim3 threadsPerBlock(2, 2, 2);``` : Here, each block contains 2 threads along the x-axis, 2 along the y-axis, and 2 along the z-axis, totaling 2×2×2=82×2×2=8 threads per block.\n",
    "    - numberOfBlocks specifies the dimensions of the grid in terms of blocks.\n",
    "    - ```dim3 numberOfBlocks(2, 2, 2);``` : The grid consists of 2 blocks along each of the three axes, resulting in 2×2×2=82×2×2=8 blocks in total.\n",
    "    - 8 threads/block * 8 blocks = 64 threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indices and dimensions of Grid, Blocks and Threads:\n",
    "    - gridDim:\n",
    "        - Size of the grid (number of blocks) in x, y, and z dimensions.\n",
    "        - Example: gridDim = dim3(4, 2, 1) for 4x2x1 blocks.\n",
    "\n",
    "    - blockDim:\n",
    "        - Size of a block (number of threads) in x, y, and z dimensions.\n",
    "        - Example: blockDim = dim3(8, 4, 1) for 8x4x1 threads.\n",
    "\n",
    "    - blockIdx:\n",
    "        - Index of the current block in the grid.\n",
    "        - Example: blockIdx = dim3(2, 1, 0) for the third block in x and second in y.\n",
    "\n",
    "    - threadIdx:\n",
    "        - Index of the current thread in the block.\n",
    "        - Example: threadIdx = dim3(3, 1, 0) for the fourth thread in x and second in y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel launch:\n",
    "    - The ```kernel_name<<<NumberOfBlocks, threadsPerBlock>>>;``` syntax is used to launch a kernel in CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In CUDA, the keywords `__global__` and `__device__` are function qualifiers that define how and where functions are executed and called. Here's a concise explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__global__`:\n",
    "    - Purpose: Marks a function as a kernel, which can be called from the host (CPU) and executed on the device (GPU).\n",
    "    - Execution:\n",
    "        - Called from: Host code.\n",
    "        - Executed on: GPU.\n",
    "    - Special Notes:\n",
    "        - Must return void.\n",
    "        - Cannot be called from another kernel or device function.\n",
    "    - Syntax: Uses triple angle brackets <<<...>>> to specify the execution configuration (grid and block dimensions) when invoking the kernel.\n",
    "    Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __global__ void add(int *a, int *b, int *c) {\n",
    "#     int idx = threadIdx.x;\n",
    "#     c[idx] = a[idx] + b[idx];\n",
    "# }\n",
    "\n",
    "# int main() {\n",
    "#     // Call kernel\n",
    "#     add<<<1, 256>>>(a, b, c);  // 1 block, 256 threads\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__device__`:\n",
    "    - Purpose: Marks a function as a device function, which is executed on the GPU and can only be called from another GPU function (e.g., another __global__ or __device__ function).\n",
    "    - Execution:\n",
    "        - Called from: GPU code.\n",
    "        - Executed on: GPU.\n",
    "    - Special Notes:\n",
    "        - Can return values.\n",
    "        - Cannot be called directly from the host.\n",
    "    - Syntax: Called like a regular function (no <<<...>>> required).\n",
    "    - Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __device__ int square(int x) {\n",
    "#     return x * x;\n",
    "# }\n",
    "\n",
    "# __global__ void calculateSquares(int *a) {\n",
    "#     int idx = threadIdx.x;\n",
    "#     a[idx] = square(idx);  // Call __device__ function\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Block ID, Block Offset, Thread ID and Global Thread ID\n",
    "    - blockID\n",
    "        - Purpose: Calculates the unique 1D ID of a block within the entire 3D grid.\n",
    "        - Explanation:\n",
    "            - blockIdx.x: Block's position in the x-dimension.\n",
    "            - blockIdx.y * gridDim.x: Adds the blocks from rows above in the grid.\n",
    "            - blockIdx.z * gridDim.x * gridDim.y: Accounts for the blocks in the z-dimension (layers).\n",
    "        - Result: A unique block ID (blockId) in the entire grid.\n",
    "\n",
    "    - blockOffset \n",
    "        - Purpose: Computes the starting global thread index of the block.\n",
    "        - Explanation:\n",
    "            - blockId: Unique block ID from Step 1.\n",
    "            - blockDim.x * blockDim.y * blockDim.z: Total number of threads in a block.\n",
    "            - Multiplying the two gives the starting position of the block in the global thread index space.\n",
    "    \n",
    "    - threadId\n",
    "        - Purpose: Computes the local thread ID (0-based index) within a block.\n",
    "        - Explanation:\n",
    "            - threadIdx.x: Thread’s position in the x-dimension within the block.\n",
    "            - threadIdx.y * blockDim.x: Adds threads from rows in the y-dimension.\n",
    "            - threadIdx.z * blockDim.x * blockDim.y: Adds threads from layers in the z-dimension.\n",
    "        - Result: A unique thread ID (threadId) within the block.\n",
    "    \n",
    "    - globalThreadID\n",
    "        - Purpose: Combines the block’s starting position (blockOffset) and the thread’s position within the block (threadId) to compute the global thread ID.\n",
    "        - Explanation:\n",
    "            - Threads in each block are indexed starting from the blockOffset.\n",
    "            - Adding threadId to blockOffset gives a unique global ID for the thread in the entire grid.\n",
    "        - Result: globalThreadId uniquely identifies a thread in the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/02_kernel_thread_and_block.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/02_kernel_thread_and_block.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **03 - Memory Allocation and Deallocation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `cudaMalloc`:\n",
    "    - Explicitly allocates memory in the GPU's global memory.\n",
    "    - `cudaMalloc(void **devPtr, size_t size);`\n",
    "        - devPtr: Pointer to the memory location on the device. A pointer-to-pointer (void **) is required because the function modifies the pointer value to point to the allocated device memory.\n",
    "        - size: Number of bytes to allocate.\n",
    "    - Necessary because the CPU and GPU have separate memory spaces.\n",
    "\n",
    "- `cudaMemcpy`:\n",
    "    - Handles data transfer between host and device.\n",
    "    - `cudaError_t cudaMemcpy(void *dst, const void *src, size_t count, kind);`\n",
    "        - dst: Destination memory address.\n",
    "        - src: Source memory address.\n",
    "        - count: Number of bytes to copy.\n",
    "        - kind: Direction of data transfer:\n",
    "            - cudaMemcpyHostToDevice: Copy data from host (CPU) to device (GPU).\n",
    "            - cudaMemcpyDeviceToHost: Copy data from device (GPU) to host (CPU).\n",
    "            - cudaMemcpyDeviceToDevice: Copy data between two device memory locations.\n",
    "    - Essential for initializing GPU computations with host data and retrieving results.\n",
    "\n",
    "\n",
    "- `cudaFree`:\n",
    "    - Frees GPU memory to avoid memory leaks.\n",
    "    - `cudaFree(void *devPtr);`\n",
    "        - devPtr: Pointer to the memory on the device to be freed.\n",
    "    - Crucial for efficient memory management in GPU applications.\n",
    "\n",
    "Together, these functions form the foundation of memory management in CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/03_memory_allocation.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make SRC=./src/03_memory_allocation.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **04 - Memory Hierarchy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Memory Hierarchy in Code\n",
    "\n",
    "1. Global Memory:\n",
    "    - `globalData` and output arrays are stored in global memory.\n",
    "    - Threads load values from global memory into registers for computation.\n",
    "    - Global memory access is slow and requires coalesced access for efficiency.\n",
    "\n",
    "2. Register Memory:\n",
    "    - `regValue` is a register variable, stored in the fastest memory on the GPU.\n",
    "    -  are private to each thread and provide the fastest access time.\n",
    "\n",
    "3. Shared Memory:\n",
    "    - `sharedMem` is shared among threads in a block.\n",
    "    - Faster than global memory but limited in size (e.g., 48 KB per SM on modern GPUs).\n",
    "    - Example: Compute the sum of all thread values in a block using shared memory.\n",
    "\n",
    "4. Local Memory:\n",
    "    - Dynamically allocated memory (`localMem`) is private to each thread.\n",
    "    - Typically stored in global memory if registers are insufficient.\n",
    "    - Used for per-thread data not shared with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "- Performance Tips:\n",
    "    - Use shared memory to minimize global memory accesses.\n",
    "    - Avoid excessive use of local memory, as it is stored in global memory and can be slow.\n",
    "- Synchronization:\n",
    "    - __syncthreads() is essential to ensure all threads have finished accessing shared memory before proceeding.\n",
    "\n",
    "This code demonstrates how memory is accessed and utilized across the GPU memory hierarchy in CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Differences Between malloc/free and cudaMalloc/cudaFree\n",
    "\n",
    "| **Aspect**               | **`malloc` / `free` in `__global__`** | **`cudaMalloc` / `cudaFree`**           |\n",
    "|--------------------------|----------------------------------------|-----------------------------------------|\n",
    "| **Scope**                | Per-thread allocation (local memory). | Device-wide allocation (global memory). |\n",
    "| **Location**             | Allocates memory from the thread’s heap (local memory). | Allocates memory in global memory.      |\n",
    "| **Usage**                | Called inside a kernel (`__global__` or `__device__`). | Called from the host.                   |\n",
    "| **Speed**                | Relatively slower due to thread-level heap management. | Faster but requires explicit host calls.|\n",
    "| **Purpose**              | Per-thread local memory, private to a thread. | Shared memory for threads, blocks, or grids. |\n",
    "| **Memory Lifetime**      | Only valid during the kernel execution. | Persistent across kernel launches (until freed). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/04_memory_hierarchy ./src/04_memory_hierarchy.cu\n",
      "././src/04_memory_hierarchy\n",
      "Block 2: Shared memory sum = 2864\n",
      "Block 1: Shared memory sum = 1840\n",
      "Block 0: Shared memory sum = 816\n",
      "Block 3: Shared memory sum = 3888\n",
      "Output from GPU:\n",
      "20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 100 102 104 106 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250 252 254 256 258 260 262 264 266 268 270 272 274 \n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/04_memory_hierarchy.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/04_memory_hierarchy\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/04_memory_hierarchy.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **05 - Synchronization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Warp-Level Synchronization\n",
    "\n",
    "    - Synchronization happens automatically within a warp (32 threads).\n",
    "    - No need for `__syncthreads()` as all threads execute in lock-step (SIMD).\n",
    "    - Limited to threads in the same warp, with no inter-warp communication.\n",
    "\n",
    "2. Block-Level Synchronization\n",
    "\n",
    "    - Synchronization within a block using `__syncthreads()` ensures all threads reach a barrier before proceeding.\n",
    "    - Threads in a block can communicate via shared memory.\n",
    "    - Cannot synchronize threads across different blocks.\n",
    "\n",
    "3. Grid-Level Synchronization\n",
    "\n",
    "    - Achieved through host intervention using `cudaDeviceSynchronize()`, ensuring all blocks complete before launching the next kernel.\n",
    "    - Involves storing intermediate results in global memory.\n",
    "    - No direct GPU-level synchronization across blocks in a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/05_synchronization ./src/05_synchronization.cu\n",
      "././src/05_synchronization\n",
      "Warp-level synchronization:\n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n",
      "\n",
      "Block-level synchronization:\n",
      "Block sums:\n",
      "Block 0 sum: 512\n",
      "Block 1 sum: 512\n",
      "\n",
      "Grid-level synchronization:\n",
      "Block 1: Updated sum after iteration 1 = 1\n",
      "Block 0: Updated sum after iteration 1 = 256\n",
      "Block 1: Updated sum after iteration 2 = 3\n",
      "Block 0: Updated sum after iteration 2 = 256\n",
      "Block 1: Updated sum after iteration 3 = 6\n",
      "Block 0: Updated sum after iteration 3 = 256\n",
      "Block 1: Updated sum after iteration 4 = 10\n",
      "Block 0: Updated sum after iteration 4 = 256\n",
      "Block 1: Updated sum after iteration 5 = 15\n",
      "Block 0: Updated sum after iteration 5 = 256\n",
      "Final block sums:\n",
      "Block 0 sum: 256\n",
      "Block 1 sum: 256\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/05_synchronization.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/05_synchronization\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/05_synchronization.cu clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **06 - Error Handling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Handling Mechanism:\n",
    "\n",
    "    - `cudaGetLastError()`:\n",
    "        - Retrieves the last error that occurred.\n",
    "        - Resets the error status to cudaSuccess for subsequent checks.\n",
    "    - `cudaGetErrorString()`:\n",
    "        - Converts a CUDA error code into a human-readable string.\n",
    "\n",
    "- Kernel Launch Error:\n",
    "\n",
    "    - The faultyKernel deliberately attempts an out-of-bounds memory access, which will cause an illegal memory access error.\n",
    "\n",
    "- Error Propagation:\n",
    "\n",
    "    - Each CUDA API call and kernel launch is followed by an error check using:\n",
    "\n",
    "    - `checkCudaError(\"Error Message\")`;\n",
    "\n",
    "- Device Synchronization:\n",
    "\n",
    "    - `cudaDeviceSynchronize()` ensures that all kernel executions and memory operations are complete, making runtime errors visible to the host.\n",
    "\n",
    "- Error Messages:\n",
    "\n",
    "    - If an error occurs, the program prints the error message and exits with EXIT_FAILURE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc -o ./src/06_error_handling ./src/06_error_handling.cu\n",
      "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/13/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
      "(.text+0x1b): undefined reference to `main'\n",
      "collect2: error: ld returned 1 exit status\n",
      "make: *** [Makefile:11: src/06_error_handling] Error 1\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/06_error_handling.cu run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -f ./src/06_error_handling\n"
     ]
    }
   ],
   "source": [
    "!make SRC=./src/06_error_handling.cu clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
